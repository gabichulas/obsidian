## Objetivo del Proyecto

Reemplazar el componente de *Reinforcement Learning* (Q-Learning) en un autoscaler para workflows cient√≠ficos en la nube, utilizando **Deep Learning** mediante **neuroevoluci√≥n de una red neuronal que act√∫a como pol√≠tica**.

---

## üß¨ ¬øQu√© es una pol√≠tica?

> Una **pol√≠tica** es una funci√≥n que, dado un estado `s`, devuelve una acci√≥n `a`.

En RL, la pol√≠tica define el comportamiento del agente en cada paso.

---

## üß† ¬øC√≥mo una red neuronal representa una pol√≠tica?

- Entrada: vector de estado (en tu caso, 4 variables categ√≥ricas o discretizadas).
- Salida: vector de scores (uno por cada acci√≥n posible).
- La acci√≥n elegida es la de mayor score (`argmax`).

Ejemplo:

```text
Estado: [join=Low, fork=High, pipe=Mid, carga=Low]
‚Üí Red neuronal ‚Üí [0.2, 1.7, -0.1]
‚Üí Acci√≥n elegida = Scaling_Type2
```

---

## üß© Enfoque actual vs tu propuesta

| Aspecto                  | RL tradicional (Q-Learning)      | Tu enfoque (Neuroevoluci√≥n)              |
|--------------------------|----------------------------------|------------------------------------------|
| Representaci√≥n de pol√≠tica | Q-table                         | Red neuronal                             |
| Aprendizaje              | Bootstrapping + TD learning      | Algoritmo evolutivo offline              |
| Entrenamiento            | Online, durante interacci√≥n      | Offline, basado en simulaciones          |
| Se√±al de entrenamiento   | Recompensa inmediata y acumulada | Makespan + Costo                         |
| Adaptabilidad online     | ‚úÖ                                | ‚ùå (pero posible h√≠brido en el futuro)    |
| Generalizaci√≥n           | limitada                         | mejor si red bien dise√±ada               |

---

## üìå Flujo del algoritmo

```text
1. Inicializar poblaci√≥n de vectores de pesos
2. Para cada individuo:
   - Construir red neuronal
   - Ejecutar simulaci√≥n con esa red como pol√≠tica
   - Medir makespan y costo
   - Asignar fitness multiobjetivo
3. Aplicar selecci√≥n, crossover, mutaci√≥n
4. Repetir por N generaciones
5. Seleccionar la mejor pol√≠tica final (m√≠nimo AggMC)
```

---

## üìö Conceptos clave

- **Neuroevoluci√≥n**: entrenamiento de redes neuronales mediante algoritmos evolutivos.
- **Pol√≠tica**: funci√≥n que define el comportamiento del agente (œÄ(s) = a).
- **AggMC**: norma L2 de (makespan, cost), utilizada como m√©trica de desempe√±o final.
- **Red neuronal como pol√≠tica**: input ‚Üí hidden layers ‚Üí scores por acci√≥n ‚Üí acci√≥n elegida.

---

## üß† Posibles extensiones

- Evoluci√≥n de arquitectura (no solo pesos).
- Integraci√≥n con RL (pol√≠tica evolucionada como inicializaci√≥n).
- Pol√≠ticas espec√≠ficas por tipo de workflow.
- Transferencia entre dominios de ejecuci√≥n.

---